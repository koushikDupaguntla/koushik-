{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":9546188,"sourceType":"datasetVersion","datasetId":5815930}],"dockerImageVersionId":30786,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import MinMaxScaler\nfrom catboost import CatBoostRegressor\nfrom sklearn.metrics import mean_squared_error\n\n# Load the data\ntrain = pd.read_csv('/kaggle/input/store-sales-time-series-forecasting/train.csv')\ntest = pd.read_csv('/kaggle/input/store-sales-time-series-forecasting/test.csv')\nstores = pd.read_csv('/kaggle/input/store-sales-time-series-forecasting/stores.csv')\noil = pd.read_csv('/kaggle/input/store-sales-time-series-forecasting/oil.csv')\nholidays = pd.read_csv('/kaggle/input/store-sales-time-series-forecasting/holidays_events.csv')\ntransactions = pd.read_csv('/kaggle/input/store-sales-time-series-forecasting/transactions.csv')\nsample_submission = pd.read_csv('/kaggle/input/store-sales-time-series-forecasting/sample_submission.csv')\n\n# Preprocessing - Keeping only data from 2016 onwards\ntrain['date'] = pd.to_datetime(train['date'])\ntest['date'] = pd.to_datetime(test['date'])\ntrain = train[train['date'] >= '2016-01-01']\n\n# Adding features for year, month, and holidays\ntrain['holiday'] = train['date'].isin(holidays['date'])\ntest['holiday'] = test['date'] == pd.to_datetime('2017-08-24')\ntrain['year'] = train['date'].dt.year\ntrain['month'] = train['date'].dt.month\ntrain['day'] = train['date'].dt.day\ntrain['weekday'] = train['date'].dt.weekday\ntest['year'] = test['date'].dt.year\ntest['month'] = test['date'].dt.month\ntest['day'] = test['date'].dt.day\ntest['weekday'] = test['date'].dt.weekday\n\n# Drop date for training and test sets\ntrain = train.drop(columns=['date'])\ntest = test.drop(columns=['date'])\n\n# One-hot encoding for categorical columns\nobject_cols = train.select_dtypes(include=['object']).columns\ntrain = pd.get_dummies(train, columns=object_cols, drop_first=True)\ntest = pd.get_dummies(test, columns=object_cols, drop_first=True)\n\n# Aligning train and test sets\ntrain, test = train.align(test, join='left', axis=1, fill_value=0)\n\n# Features and target setup\nX = train.drop(columns=['sales'])\ny = train['sales']\n\n# Log transformation of the target to reduce the effect of outliers\ny_log = np.log1p(y)\n\n# Train-validation split\nX_train, X_val, y_train, y_val = train_test_split(X, y_log, test_size=0.2, random_state=42)\n\n# Scaling\nscaler_X = MinMaxScaler()\nscaler_y = MinMaxScaler()\n\nX_train_scaled = scaler_X.fit_transform(X_train)\nX_val_scaled = scaler_X.transform(X_val)\ny_train_scaled = scaler_y.fit_transform(y_train.values.reshape(-1, 1))\ny_val_scaled = scaler_y.transform(y_val.values.reshape(-1, 1))\n\n# Reshaping data for LSTM input\nX_train_scaled = X_train_scaled.reshape(X_train_scaled.shape[0], 1, X_train_scaled.shape[1])\nX_val_scaled = X_val_scaled.reshape(X_val_scaled.shape[0], 1, X_val_scaled.shape[1])\n\n# Converting data to PyTorch tensors \nX_train_tensor = torch.tensor(X_train_scaled, dtype=torch.float32)\nX_val_tensor = torch.tensor(X_val_scaled, dtype=torch.float32)\ny_train_tensor = torch.tensor(y_train_scaled, dtype=torch.float32)\ny_val_tensor = torch.tensor(y_val_scaled, dtype=torch.float32)\n\n# Define the LSTM model in PyTorch\nclass LSTMModel(nn.Module):\n    def __init__(self, input_size, hidden_size, num_layers, output_size):\n        super(LSTMModel, self).__init__()\n        self.hidden_size = hidden_size\n        self.num_layers = num_layers\n        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n        self.fc = nn.Linear(hidden_size, output_size)\n    \n    def forward(self, x):\n        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size)  \n        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size)  \n        out, _ = self.lstm(x, (h0, c0))\n        out = self.fc(out[:, -1, :])\n        return out\n\n# LSTM Model parameters\ninput_size = X_train_tensor.shape[2]\nhidden_size = 50\nnum_layers = 2\noutput_size = 1\n\n# Instantiate the LSTM model, define loss and optimizer\nlstm_model = LSTMModel(input_size, hidden_size, num_layers, output_size)\nloss_function = nn.MSELoss()\noptimizer = optim.Adam(lstm_model.parameters(), lr=0.001)\n\n# Training the LSTM model\nnum_epochs = 50\nfor epoch in range(num_epochs):\n    lstm_model.train()\n    optimizer.zero_grad()\n    y_train_pred = lstm_model(X_train_tensor)\n    loss = loss_function(y_train_pred, y_train_tensor)\n    loss.backward()\n    optimizer.step()\n\n    # Validation step\n    lstm_model.eval()\n    with torch.no_grad():\n        y_val_pred = lstm_model(X_val_tensor)\n        val_loss = loss_function(y_val_pred, y_val_tensor)\n        y_val_pred_inverse = scaler_y.inverse_transform(y_val_pred.numpy())\n        y_val_inverse = scaler_y.inverse_transform(y_val_tensor.numpy())\n        val_rmsle = calculate_rmsle(np.expm1(y_val_inverse), np.expm1(y_val_pred_inverse))\n    \n    print(f'Epoch {epoch+1}/{num_epochs}, Training Loss: {loss.item()}, Validation Loss: {val_loss.item()}, Validation RMSLE: {val_rmsle}')\n\n# Extracting features from LSTM\nwith torch.no_grad():\n    lstm_train_features = lstm_model(X_train_tensor).numpy()\n    lstm_val_features = lstm_model(X_val_tensor).numpy()\n\n# Combining LSTM features with original dataset\ntrain_lstm_df = pd.DataFrame(lstm_train_features, columns=[f'lstm_feat_{i}' for i in range(lstm_train_features.shape[1])])\nval_lstm_df = pd.DataFrame(lstm_val_features, columns=[f'lstm_feat_{i}' for i in range(lstm_val_features.shape[1])])\n\nX_train_combined = pd.concat([X_train.reset_index(drop=True), train_lstm_df], axis=1)\nX_val_combined = pd.concat([X_val.reset_index(drop=True), val_lstm_df], axis=1)\n\n# Setting up CatBoost model with fixed parameters (iterations=1000, depth=8, learning_rate=0.1)\ncatboost_model = CatBoostRegressor(\n    iterations=1000,\n    depth=8,\n    learning_rate=0.1,\n    loss_function='RMSE',\n    verbose=100\n)\n\n# Training CatBoost model\ncatboost_model.fit(X_train_combined, y_train, eval_set=(X_val_combined, y_val), early_stopping_rounds=50)\n\n# Predictions and evaluation\ny_val_pred_catboost = catboost_model.predict(X_val_combined)\nval_rmsle_final = calculate_rmsle(np.expm1(y_val), np.expm1(y_val_pred_catboost))\nprint(f'Final Validation RMSLE: {val_rmsle_final}')\n\n# Test data predictions\nX_test_scaled = scaler_X.transform(test.drop(columns=['sales'], errors='ignore'))  \nX_test_scaled = X_test_scaled.reshape(X_test_scaled.shape[0], 1, X_test_scaled.shape[1])\nX_test_tensor = torch.tensor(X_test_scaled, dtype=torch.float32)\n\n# RMSLE function\ndef calculate_rmsle(y_true, y_pred):\n    return np.sqrt(np.mean(np.square(np.log1p(y_pred) - np.log1p(y_true))))\n\n\n# Extracting LSTM features for test data\nwith torch.no_grad():\n    lstm_test_features = lstm_model(X_test_tensor).numpy()\n\n# Combining LSTM features with the original test features\ntest_lstm_df = pd.DataFrame(lstm_test_features, columns=[f'lstm_feat_{i}' for i in range(lstm_test_features.shape[1])])\nX_test_combined = pd.concat([test.reset_index(drop=True), test_lstm_df], axis=1)\n\n# Making predictions on the test set with the CatBoost model\ntest_predictions_catboost = catboost_model.predict(X_test_combined)\n\n# Converting predictions back from log1p scale\ntest['sales'] = np.expm1(test_predictions_catboost)\n\n# Handling negative values in predictions (if any)\ntest['sales'] = np.where(test['sales'] < 0, 0, test['sales'])\n\n# submission file\nsubmission = test[['id', 'sales']]\nsubmission.to_csv('submission_new.csv', index=False)\nprint(\"Submission file generated.\")\n","metadata":{"execution":{"iopub.status.busy":"2024-10-04T10:41:37.396423Z","iopub.execute_input":"2024-10-04T10:41:37.396862Z","iopub.status.idle":"2024-10-04T10:53:27.929728Z","shell.execute_reply.started":"2024-10-04T10:41:37.396819Z","shell.execute_reply":"2024-10-04T10:53:27.928562Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stderr","text":"/tmp/ipykernel_30/1894335444.py:30: FutureWarning: The behavior of 'isin' with dtype=datetime64[ns] and castable values (e.g. strings) is deprecated. In a future version, these will not be considered matching by isin. Explicitly cast to the appropriate dtype before calling isin instead.\n  train['holiday'] = train['date'].isin(holidays['date'])\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1/50, Training Loss: 0.11507447063922882, Validation Loss: 0.11152812093496323, Validation RMSLE: 3.91860294342041\nEpoch 2/50, Training Loss: 0.11169958114624023, Validation Loss: 0.10825351625680923, Validation RMSLE: 3.860646963119507\nEpoch 3/50, Training Loss: 0.10842482000589371, Validation Loss: 0.1050691083073616, Validation RMSLE: 3.803440570831299\nEpoch 4/50, Training Loss: 0.10524023324251175, Validation Loss: 0.10196571797132492, Validation RMSLE: 3.7468485832214355\nEpoch 5/50, Training Loss: 0.10213664174079895, Validation Loss: 0.09893510490655899, Validation RMSLE: 3.690746784210205\nEpoch 6/50, Training Loss: 0.0991058275103569, Validation Loss: 0.0959700495004654, Validation RMSLE: 3.6350207328796387\nEpoch 7/50, Training Loss: 0.09614058583974838, Validation Loss: 0.09306420385837555, Validation RMSLE: 3.57956600189209\nEpoch 8/50, Training Loss: 0.09323455393314362, Validation Loss: 0.09021230787038803, Validation RMSLE: 3.524292469024658\nEpoch 9/50, Training Loss: 0.09038244187831879, Validation Loss: 0.08741021156311035, Validation RMSLE: 3.4691264629364014\nEpoch 10/50, Training Loss: 0.08758009970188141, Validation Loss: 0.08465501666069031, Validation RMSLE: 3.4140148162841797\nEpoch 11/50, Training Loss: 0.08482465147972107, Validation Loss: 0.08194510638713837, Validation RMSLE: 3.3589272499084473\nEpoch 12/50, Training Loss: 0.08211445808410645, Validation Loss: 0.07927999645471573, Validation RMSLE: 3.30385422706604\nEpoch 13/50, Training Loss: 0.07944902777671814, Validation Loss: 0.07666028290987015, Validation RMSLE: 3.248809814453125\nEpoch 14/50, Training Loss: 0.0768289566040039, Validation Loss: 0.07408766448497772, Validation RMSLE: 3.1938319206237793\nEpoch 15/50, Training Loss: 0.07425593584775925, Validation Loss: 0.07156491279602051, Validation RMSLE: 3.138984441757202\nEpoch 16/50, Training Loss: 0.07173273712396622, Validation Loss: 0.0690959244966507, Validation RMSLE: 3.0843617916107178\nEpoch 17/50, Training Loss: 0.06926323473453522, Validation Loss: 0.06668578088283539, Validation RMSLE: 3.0300912857055664\nEpoch 18/50, Training Loss: 0.06685251742601395, Validation Loss: 0.06434082239866257, Validation RMSLE: 2.9763386249542236\nEpoch 19/50, Training Loss: 0.0645069032907486, Validation Loss: 0.06206874921917915, Validation RMSLE: 2.9233148097991943\nEpoch 20/50, Training Loss: 0.062234099954366684, Validation Loss: 0.05987870320677757, Validation RMSLE: 2.8712782859802246\nEpoch 21/50, Training Loss: 0.06004324555397034, Validation Loss: 0.05778125301003456, Validation RMSLE: 2.8205418586730957\nEpoch 22/50, Training Loss: 0.05794486775994301, Validation Loss: 0.055788248777389526, Validation RMSLE: 2.7714715003967285\nEpoch 23/50, Training Loss: 0.055950816720724106, Validation Loss: 0.053912680596113205, Validation RMSLE: 2.7244856357574463\nEpoch 24/50, Training Loss: 0.05407405644655228, Validation Loss: 0.05216827243566513, Validation RMSLE: 2.680046558380127\nEpoch 25/50, Training Loss: 0.05232834070920944, Validation Loss: 0.050569117069244385, Validation RMSLE: 2.6386494636535645\nEpoch 26/50, Training Loss: 0.050727687776088715, Validation Loss: 0.04912884905934334, Validation RMSLE: 2.600802421569824\nEpoch 27/50, Training Loss: 0.049285758286714554, Validation Loss: 0.04785989224910736, Validation RMSLE: 2.5669946670532227\nEpoch 28/50, Training Loss: 0.048014964908361435, Validation Loss: 0.04677227512001991, Validation RMSLE: 2.5376594066619873\nEpoch 29/50, Training Loss: 0.046925321221351624, Validation Loss: 0.04587224870920181, Validation RMSLE: 2.513124942779541\nEpoch 30/50, Training Loss: 0.04602307826280594, Validation Loss: 0.04516063258051872, Validation RMSLE: 2.493555784225464\nEpoch 31/50, Training Loss: 0.04530908912420273, Validation Loss: 0.04463103413581848, Validation RMSLE: 2.478891372680664\nEpoch 32/50, Training Loss: 0.04477698355913162, Validation Loss: 0.044268228113651276, Validation RMSLE: 2.4687955379486084\nEpoch 33/50, Training Loss: 0.044411588460206985, Validation Loss: 0.044047143310308456, Validation RMSLE: 2.462623119354248\nEpoch 34/50, Training Loss: 0.04418787360191345, Validation Loss: 0.04393301159143448, Validation RMSLE: 2.459430456161499\nEpoch 35/50, Training Loss: 0.044071171432733536, Validation Loss: 0.04388348013162613, Validation RMSLE: 2.4580438137054443\nEpoch 36/50, Training Loss: 0.04401920363306999, Validation Loss: 0.04385276138782501, Validation RMSLE: 2.4571831226348877\nEpoch 37/50, Training Loss: 0.04398626834154129, Validation Loss: 0.0437973253428936, Validation RMSLE: 2.455629348754883\nEpoch 38/50, Training Loss: 0.043928924947977066, Validation Loss: 0.04368177056312561, Validation RMSLE: 2.452387809753418\nEpoch 39/50, Training Loss: 0.04381181672215462, Validation Loss: 0.04348313435912132, Validation RMSLE: 2.446805477142334\nEpoch 40/50, Training Loss: 0.043612007051706314, Validation Loss: 0.04319252818822861, Validation RMSLE: 2.4386160373687744\nEpoch 41/50, Training Loss: 0.04332062229514122, Validation Loss: 0.04281412065029144, Validation RMSLE: 2.4279098510742188\nEpoch 42/50, Training Loss: 0.042941778898239136, Validation Loss: 0.0423620380461216, Validation RMSLE: 2.41505765914917\nEpoch 43/50, Training Loss: 0.04248957335948944, Validation Loss: 0.041856393218040466, Validation RMSLE: 2.4006009101867676\nEpoch 44/50, Training Loss: 0.04198405519127846, Validation Loss: 0.04131929948925972, Validation RMSLE: 2.3851490020751953\nEpoch 45/50, Training Loss: 0.04144728183746338, Validation Loss: 0.04077158123254776, Validation RMSLE: 2.3692879676818848\nEpoch 46/50, Training Loss: 0.04090002551674843, Validation Loss: 0.04023047909140587, Validation RMSLE: 2.353513240814209\nEpoch 47/50, Training Loss: 0.04035944119095802, Validation Loss: 0.039708346128463745, Validation RMSLE: 2.338191032409668\nEpoch 48/50, Training Loss: 0.03983785957098007, Validation Loss: 0.03921233490109444, Validation RMSLE: 2.3235416412353516\nEpoch 49/50, Training Loss: 0.039342381060123444, Validation Loss: 0.038744740188121796, Validation RMSLE: 2.3096461296081543\nEpoch 50/50, Training Loss: 0.0388752706348896, Validation Loss: 0.038303811103105545, Validation RMSLE: 2.296466112136841\n0:\tlearn: 2.4012583\ttest: 2.3975518\tbest: 2.3975518 (0)\ttotal: 166ms\tremaining: 2m 45s\n100:\tlearn: 0.7451434\ttest: 0.7419289\tbest: 0.7419289 (100)\ttotal: 8.94s\tremaining: 1m 19s\n200:\tlearn: 0.6389866\ttest: 0.6357778\tbest: 0.6357778 (200)\ttotal: 18.2s\tremaining: 1m 12s\n300:\tlearn: 0.5864447\ttest: 0.5844569\tbest: 0.5844569 (300)\ttotal: 27s\tremaining: 1m 2s\n400:\tlearn: 0.5536217\ttest: 0.5528317\tbest: 0.5528317 (400)\ttotal: 35.8s\tremaining: 53.5s\n500:\tlearn: 0.5313648\ttest: 0.5309542\tbest: 0.5309542 (500)\ttotal: 44.5s\tremaining: 44.4s\n600:\tlearn: 0.5128055\ttest: 0.5132167\tbest: 0.5132167 (600)\ttotal: 54s\tremaining: 35.9s\n700:\tlearn: 0.4988535\ttest: 0.4997283\tbest: 0.4997283 (700)\ttotal: 1m 2s\tremaining: 26.8s\n800:\tlearn: 0.4881252\ttest: 0.4894758\tbest: 0.4894758 (800)\ttotal: 1m 11s\tremaining: 17.8s\n900:\tlearn: 0.4791126\ttest: 0.4809457\tbest: 0.4809457 (900)\ttotal: 1m 20s\tremaining: 8.86s\n999:\tlearn: 0.4705617\ttest: 0.4728030\tbest: 0.4728030 (999)\ttotal: 1m 29s\tremaining: 0us\n\nbestTest = 0.4728029649\nbestIteration = 999\n\nFinal Validation RMSLE: 0.4728029636233947\nSubmission file generated.\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}
